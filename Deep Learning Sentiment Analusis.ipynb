{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-29T19:34:46.625313Z",
     "start_time": "2024-06-29T19:34:41.262189Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:34:47.300759Z",
     "start_time": "2024-06-29T19:34:46.627319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the data\n",
    "filename = \"archive/IMDB Dataset.csv\"\n",
    "sample_data = []\n",
    "file = open(filename, encoding=\"utf8\")\n",
    "csv_reader = csv.reader(file)\n",
    "for row in csv_reader:\n",
    "    sample_data.append(row)\n",
    "file.close()"
   ],
   "id": "7a16de857cd0394",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:34:47.316757Z",
     "start_time": "2024-06-29T19:34:47.302757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_Values = []\n",
    "Y_Values = []"
   ],
   "id": "f0df13c8e1776d28",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:34:47.347465Z",
     "start_time": "2024-06-29T19:34:47.318759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1, len(sample_data)):\n",
    "    X_Values.append(sample_data[i][0])\n",
    "    Y_Values.append(sample_data[i][1])"
   ],
   "id": "9c7096fa4635a400",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:34:47.363264Z",
     "start_time": "2024-06-29T19:34:47.350636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert the Y_Values to 0 and 1\n",
    "for i in range(len(Y_Values)):\n",
    "    if Y_Values[i] == \"positive\":\n",
    "        Y_Values[i] = 1\n",
    "    else:\n",
    "        Y_Values[i] = 0"
   ],
   "id": "399b1f79a196578c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:34:48.200388Z",
     "start_time": "2024-06-29T19:34:47.364879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert both arrays to numpy arrays\n",
    "X_Values = np.array(X_Values)\n",
    "Y_Values = np.array(Y_Values)"
   ],
   "id": "4f44327933cfdfed",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:34:57.711485Z",
     "start_time": "2024-06-29T19:34:48.201281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Removing the unwanted Tags.\n",
    "X_Values = np.array([re.sub('<.*?>', ' ', data) for data in X_Values])\n",
    "# Removing the special characters.\n",
    "X_Values = np.array([re.sub('[^a-zA-Z0-9\\s]', ' ', data) for data in X_Values])\n",
    "# Removing the extra spaces.\n",
    "X_Values = np.array([' '.join(data.split()) for data in X_Values])"
   ],
   "id": "23ea4fdb002d1b25",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:34:58.612946Z",
     "start_time": "2024-06-29T19:34:57.713381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_Values, Y_Values, test_size=0.2, random_state=42)"
   ],
   "id": "31fb174c5a5e25b2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:35:19.055846Z",
     "start_time": "2024-06-29T19:34:58.614929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize the data\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 200\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "x_train = tokenizer.texts_to_sequences(X_train)\n",
    "x_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train_pad = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_pad = pad_sequences(x_test, maxlen=maxlen)\n"
   ],
   "id": "fdb295760191c7d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:35:19.071830Z",
     "start_time": "2024-06-29T19:35:19.056841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a custom dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "id": "8e513fbcfae765e6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:35:19.087842Z",
     "start_time": "2024-06-29T19:35:19.076847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the dataloaders\n",
    "train_dataset = SentimentDataset(x_train_pad, y_train)\n",
    "test_dataset = SentimentDataset(x_test_pad, y_test)"
   ],
   "id": "d1d2e4d2cce70d6e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:35:19.103833Z",
     "start_time": "2024-06-29T19:35:19.089844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "id": "d7dcc5c0a84f34b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:35:19.119836Z",
     "start_time": "2024-06-29T19:35:19.105836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "input_size = max_features\n",
    "embedding_dim = 128\n",
    "hidden_size = 100\n",
    "num_layers = 6\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 250\n",
    "batch_size = 64"
   ],
   "id": "44d4225343ffdf71",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:35:21.632352Z",
     "start_time": "2024-06-29T19:35:19.123839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = RNN(input_size, embedding_dim, hidden_size, num_layers, output_size).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "id": "291b02a55b136fd3",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:35:21.647951Z",
     "start_time": "2024-06-29T19:35:21.633824Z"
    }
   },
   "cell_type": "code",
   "source": "LossScore = []",
   "id": "d7c2d482c6675716",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T20:05:17.383652Z",
     "start_time": "2024-06-29T19:35:21.650962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(train_dataset), batch_size):\n",
    "        x_batch, y_batch = train_dataset[i:i+batch_size]\n",
    "        x_batch = torch.tensor(x_batch).to(device).long()\n",
    "        y_batch = torch.tensor(y_batch).to(device).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_batch).squeeze()  # Squeeze the output\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataset)//batch_size}], Loss: {loss.item():.4f}\")\n",
    "    LossScore.append(loss.item())"
   ],
   "id": "a524cd08cdceb5f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250], Step [39937/625], Loss: 0.6508\n",
      "Epoch [2/250], Step [39937/625], Loss: 0.6872\n",
      "Epoch [3/250], Step [39937/625], Loss: 0.5908\n",
      "Epoch [4/250], Step [39937/625], Loss: 0.4779\n",
      "Epoch [5/250], Step [39937/625], Loss: 0.6901\n",
      "Epoch [6/250], Step [39937/625], Loss: 0.4816\n",
      "Epoch [7/250], Step [39937/625], Loss: 0.4719\n",
      "Epoch [8/250], Step [39937/625], Loss: 0.4875\n",
      "Epoch [9/250], Step [39937/625], Loss: 0.3378\n",
      "Epoch [10/250], Step [39937/625], Loss: 0.3886\n",
      "Epoch [11/250], Step [39937/625], Loss: 0.6954\n",
      "Epoch [12/250], Step [39937/625], Loss: 0.3789\n",
      "Epoch [13/250], Step [39937/625], Loss: 0.3569\n",
      "Epoch [14/250], Step [39937/625], Loss: 0.3723\n",
      "Epoch [15/250], Step [39937/625], Loss: 0.3159\n",
      "Epoch [16/250], Step [39937/625], Loss: 0.2764\n",
      "Epoch [17/250], Step [39937/625], Loss: 0.2977\n",
      "Epoch [18/250], Step [39937/625], Loss: 0.3105\n",
      "Epoch [19/250], Step [39937/625], Loss: 0.3586\n",
      "Epoch [20/250], Step [39937/625], Loss: 0.2770\n",
      "Epoch [21/250], Step [39937/625], Loss: 0.2942\n",
      "Epoch [22/250], Step [39937/625], Loss: 0.2644\n",
      "Epoch [23/250], Step [39937/625], Loss: 0.3995\n",
      "Epoch [24/250], Step [39937/625], Loss: 0.3175\n",
      "Epoch [25/250], Step [39937/625], Loss: 0.4197\n",
      "Epoch [26/250], Step [39937/625], Loss: 0.3032\n",
      "Epoch [27/250], Step [39937/625], Loss: 0.2448\n",
      "Epoch [28/250], Step [39937/625], Loss: 0.2489\n",
      "Epoch [29/250], Step [39937/625], Loss: 0.2800\n",
      "Epoch [30/250], Step [39937/625], Loss: 0.2710\n",
      "Epoch [31/250], Step [39937/625], Loss: 0.3218\n",
      "Epoch [32/250], Step [39937/625], Loss: 0.4228\n",
      "Epoch [33/250], Step [39937/625], Loss: 0.3339\n",
      "Epoch [34/250], Step [39937/625], Loss: 0.2812\n",
      "Epoch [35/250], Step [39937/625], Loss: 0.4357\n",
      "Epoch [36/250], Step [39937/625], Loss: 0.4454\n",
      "Epoch [37/250], Step [39937/625], Loss: 0.2338\n",
      "Epoch [38/250], Step [39937/625], Loss: 0.2331\n",
      "Epoch [39/250], Step [39937/625], Loss: 0.3212\n",
      "Epoch [40/250], Step [39937/625], Loss: 0.3547\n",
      "Epoch [41/250], Step [39937/625], Loss: 0.3384\n",
      "Epoch [42/250], Step [39937/625], Loss: 0.3320\n",
      "Epoch [43/250], Step [39937/625], Loss: 0.3290\n",
      "Epoch [44/250], Step [39937/625], Loss: 0.3135\n",
      "Epoch [45/250], Step [39937/625], Loss: 0.4522\n",
      "Epoch [46/250], Step [39937/625], Loss: 0.3264\n",
      "Epoch [47/250], Step [39937/625], Loss: 0.2668\n",
      "Epoch [48/250], Step [39937/625], Loss: 0.4305\n",
      "Epoch [49/250], Step [39937/625], Loss: 0.4047\n",
      "Epoch [50/250], Step [39937/625], Loss: 0.4944\n",
      "Epoch [51/250], Step [39937/625], Loss: 0.4282\n",
      "Epoch [52/250], Step [39937/625], Loss: 0.3837\n",
      "Epoch [53/250], Step [39937/625], Loss: 0.4849\n",
      "Epoch [54/250], Step [39937/625], Loss: 0.4342\n",
      "Epoch [55/250], Step [39937/625], Loss: 0.4347\n",
      "Epoch [56/250], Step [39937/625], Loss: 0.3259\n",
      "Epoch [57/250], Step [39937/625], Loss: 0.2848\n",
      "Epoch [58/250], Step [39937/625], Loss: 0.3114\n",
      "Epoch [59/250], Step [39937/625], Loss: 0.2891\n",
      "Epoch [60/250], Step [39937/625], Loss: 0.2793\n",
      "Epoch [61/250], Step [39937/625], Loss: 0.2797\n",
      "Epoch [62/250], Step [39937/625], Loss: 0.2573\n",
      "Epoch [63/250], Step [39937/625], Loss: 0.2008\n",
      "Epoch [64/250], Step [39937/625], Loss: 0.4889\n",
      "Epoch [65/250], Step [39937/625], Loss: 0.2848\n",
      "Epoch [66/250], Step [39937/625], Loss: 0.2749\n",
      "Epoch [67/250], Step [39937/625], Loss: 0.2113\n",
      "Epoch [68/250], Step [39937/625], Loss: 0.1873\n",
      "Epoch [69/250], Step [39937/625], Loss: 0.2482\n",
      "Epoch [70/250], Step [39937/625], Loss: 0.2682\n",
      "Epoch [71/250], Step [39937/625], Loss: 0.2472\n",
      "Epoch [72/250], Step [39937/625], Loss: 0.3265\n",
      "Epoch [73/250], Step [39937/625], Loss: 0.2358\n",
      "Epoch [74/250], Step [39937/625], Loss: 0.2198\n",
      "Epoch [75/250], Step [39937/625], Loss: 0.2985\n",
      "Epoch [76/250], Step [39937/625], Loss: 0.2390\n",
      "Epoch [77/250], Step [39937/625], Loss: 0.3217\n",
      "Epoch [78/250], Step [39937/625], Loss: 0.2802\n",
      "Epoch [79/250], Step [39937/625], Loss: 0.2795\n",
      "Epoch [80/250], Step [39937/625], Loss: 0.1973\n",
      "Epoch [81/250], Step [39937/625], Loss: 0.2540\n",
      "Epoch [82/250], Step [39937/625], Loss: 0.2725\n",
      "Epoch [83/250], Step [39937/625], Loss: 0.2264\n",
      "Epoch [84/250], Step [39937/625], Loss: 0.3016\n",
      "Epoch [85/250], Step [39937/625], Loss: 0.2781\n",
      "Epoch [86/250], Step [39937/625], Loss: 0.2273\n",
      "Epoch [87/250], Step [39937/625], Loss: 0.2072\n",
      "Epoch [88/250], Step [39937/625], Loss: 0.1916\n",
      "Epoch [89/250], Step [39937/625], Loss: 0.2013\n",
      "Epoch [90/250], Step [39937/625], Loss: 0.1673\n",
      "Epoch [91/250], Step [39937/625], Loss: 0.1927\n",
      "Epoch [92/250], Step [39937/625], Loss: 0.1698\n",
      "Epoch [93/250], Step [39937/625], Loss: 0.2173\n",
      "Epoch [94/250], Step [39937/625], Loss: 0.1873\n",
      "Epoch [95/250], Step [39937/625], Loss: 0.1491\n",
      "Epoch [96/250], Step [39937/625], Loss: 0.1910\n",
      "Epoch [97/250], Step [39937/625], Loss: 0.1489\n",
      "Epoch [98/250], Step [39937/625], Loss: 0.2076\n",
      "Epoch [99/250], Step [39937/625], Loss: 0.2421\n",
      "Epoch [100/250], Step [39937/625], Loss: 0.1805\n",
      "Epoch [101/250], Step [39937/625], Loss: 0.2395\n",
      "Epoch [102/250], Step [39937/625], Loss: 0.2332\n",
      "Epoch [103/250], Step [39937/625], Loss: 0.2212\n",
      "Epoch [104/250], Step [39937/625], Loss: 0.2040\n",
      "Epoch [105/250], Step [39937/625], Loss: 0.1957\n",
      "Epoch [106/250], Step [39937/625], Loss: 0.1599\n",
      "Epoch [107/250], Step [39937/625], Loss: 0.1797\n",
      "Epoch [108/250], Step [39937/625], Loss: 0.1618\n",
      "Epoch [109/250], Step [39937/625], Loss: 0.1894\n",
      "Epoch [110/250], Step [39937/625], Loss: 0.1591\n",
      "Epoch [111/250], Step [39937/625], Loss: 0.1482\n",
      "Epoch [112/250], Step [39937/625], Loss: 0.1460\n",
      "Epoch [113/250], Step [39937/625], Loss: 0.1627\n",
      "Epoch [114/250], Step [39937/625], Loss: 0.1942\n",
      "Epoch [115/250], Step [39937/625], Loss: 0.1695\n",
      "Epoch [116/250], Step [39937/625], Loss: 0.2378\n",
      "Epoch [117/250], Step [39937/625], Loss: 0.1210\n",
      "Epoch [118/250], Step [39937/625], Loss: 0.1496\n",
      "Epoch [119/250], Step [39937/625], Loss: 0.1071\n",
      "Epoch [120/250], Step [39937/625], Loss: 0.2413\n",
      "Epoch [121/250], Step [39937/625], Loss: 0.2371\n",
      "Epoch [122/250], Step [39937/625], Loss: 0.2006\n",
      "Epoch [123/250], Step [39937/625], Loss: 0.1989\n",
      "Epoch [124/250], Step [39937/625], Loss: 0.1828\n",
      "Epoch [125/250], Step [39937/625], Loss: 0.1849\n",
      "Epoch [126/250], Step [39937/625], Loss: 0.2499\n",
      "Epoch [127/250], Step [39937/625], Loss: 0.1841\n",
      "Epoch [128/250], Step [39937/625], Loss: 0.1731\n",
      "Epoch [129/250], Step [39937/625], Loss: 0.1189\n",
      "Epoch [130/250], Step [39937/625], Loss: 0.1664\n",
      "Epoch [131/250], Step [39937/625], Loss: 0.1568\n",
      "Epoch [132/250], Step [39937/625], Loss: 0.1086\n",
      "Epoch [133/250], Step [39937/625], Loss: 0.2082\n",
      "Epoch [134/250], Step [39937/625], Loss: 0.1043\n",
      "Epoch [135/250], Step [39937/625], Loss: 0.1398\n",
      "Epoch [136/250], Step [39937/625], Loss: 0.0906\n",
      "Epoch [137/250], Step [39937/625], Loss: 0.0618\n",
      "Epoch [138/250], Step [39937/625], Loss: 0.1696\n",
      "Epoch [139/250], Step [39937/625], Loss: 0.2315\n",
      "Epoch [140/250], Step [39937/625], Loss: 0.1327\n",
      "Epoch [141/250], Step [39937/625], Loss: 0.0803\n",
      "Epoch [142/250], Step [39937/625], Loss: 0.1117\n",
      "Epoch [143/250], Step [39937/625], Loss: 0.1533\n",
      "Epoch [144/250], Step [39937/625], Loss: 0.1144\n",
      "Epoch [145/250], Step [39937/625], Loss: 0.2008\n",
      "Epoch [146/250], Step [39937/625], Loss: 0.2102\n",
      "Epoch [147/250], Step [39937/625], Loss: 0.1336\n",
      "Epoch [148/250], Step [39937/625], Loss: 0.1400\n",
      "Epoch [149/250], Step [39937/625], Loss: 0.1185\n",
      "Epoch [150/250], Step [39937/625], Loss: 0.1720\n",
      "Epoch [151/250], Step [39937/625], Loss: 0.1134\n",
      "Epoch [152/250], Step [39937/625], Loss: 0.0821\n",
      "Epoch [153/250], Step [39937/625], Loss: 0.0871\n",
      "Epoch [154/250], Step [39937/625], Loss: 0.0895\n",
      "Epoch [155/250], Step [39937/625], Loss: 0.1608\n",
      "Epoch [156/250], Step [39937/625], Loss: 0.1845\n",
      "Epoch [157/250], Step [39937/625], Loss: 0.1489\n",
      "Epoch [158/250], Step [39937/625], Loss: 0.1555\n",
      "Epoch [159/250], Step [39937/625], Loss: 0.2128\n",
      "Epoch [160/250], Step [39937/625], Loss: 0.2288\n",
      "Epoch [161/250], Step [39937/625], Loss: 0.1776\n",
      "Epoch [162/250], Step [39937/625], Loss: 0.1241\n",
      "Epoch [163/250], Step [39937/625], Loss: 0.1471\n",
      "Epoch [164/250], Step [39937/625], Loss: 0.1750\n",
      "Epoch [165/250], Step [39937/625], Loss: 0.1687\n",
      "Epoch [166/250], Step [39937/625], Loss: 0.2364\n",
      "Epoch [167/250], Step [39937/625], Loss: 0.1715\n",
      "Epoch [168/250], Step [39937/625], Loss: 0.2216\n",
      "Epoch [169/250], Step [39937/625], Loss: 0.2079\n",
      "Epoch [170/250], Step [39937/625], Loss: 0.2592\n",
      "Epoch [171/250], Step [39937/625], Loss: 0.2215\n",
      "Epoch [172/250], Step [39937/625], Loss: 0.2162\n",
      "Epoch [173/250], Step [39937/625], Loss: 0.1365\n",
      "Epoch [174/250], Step [39937/625], Loss: 0.1907\n",
      "Epoch [175/250], Step [39937/625], Loss: 0.1693\n",
      "Epoch [176/250], Step [39937/625], Loss: 0.2258\n",
      "Epoch [177/250], Step [39937/625], Loss: 0.1915\n",
      "Epoch [178/250], Step [39937/625], Loss: 0.1777\n",
      "Epoch [179/250], Step [39937/625], Loss: 0.0835\n",
      "Epoch [180/250], Step [39937/625], Loss: 0.0579\n",
      "Epoch [181/250], Step [39937/625], Loss: 0.1425\n",
      "Epoch [182/250], Step [39937/625], Loss: 0.1181\n",
      "Epoch [183/250], Step [39937/625], Loss: 0.1129\n",
      "Epoch [184/250], Step [39937/625], Loss: 0.0666\n",
      "Epoch [185/250], Step [39937/625], Loss: 0.0830\n",
      "Epoch [186/250], Step [39937/625], Loss: 0.0921\n",
      "Epoch [187/250], Step [39937/625], Loss: 0.0839\n",
      "Epoch [188/250], Step [39937/625], Loss: 0.0988\n",
      "Epoch [189/250], Step [39937/625], Loss: 0.0782\n",
      "Epoch [190/250], Step [39937/625], Loss: 0.1277\n",
      "Epoch [191/250], Step [39937/625], Loss: 0.1801\n",
      "Epoch [192/250], Step [39937/625], Loss: 0.0821\n",
      "Epoch [193/250], Step [39937/625], Loss: 0.1299\n",
      "Epoch [194/250], Step [39937/625], Loss: 0.1702\n",
      "Epoch [195/250], Step [39937/625], Loss: 0.1105\n",
      "Epoch [196/250], Step [39937/625], Loss: 0.2195\n",
      "Epoch [197/250], Step [39937/625], Loss: 0.1240\n",
      "Epoch [198/250], Step [39937/625], Loss: 0.1241\n",
      "Epoch [199/250], Step [39937/625], Loss: 0.0938\n",
      "Epoch [200/250], Step [39937/625], Loss: 0.0981\n",
      "Epoch [201/250], Step [39937/625], Loss: 0.1679\n",
      "Epoch [202/250], Step [39937/625], Loss: 0.1917\n",
      "Epoch [203/250], Step [39937/625], Loss: 0.1216\n",
      "Epoch [204/250], Step [39937/625], Loss: 0.1859\n",
      "Epoch [205/250], Step [39937/625], Loss: 0.1423\n",
      "Epoch [206/250], Step [39937/625], Loss: 0.1193\n",
      "Epoch [207/250], Step [39937/625], Loss: 0.2155\n",
      "Epoch [208/250], Step [39937/625], Loss: 0.1674\n",
      "Epoch [209/250], Step [39937/625], Loss: 0.2209\n",
      "Epoch [210/250], Step [39937/625], Loss: 0.1593\n",
      "Epoch [211/250], Step [39937/625], Loss: 0.2589\n",
      "Epoch [212/250], Step [39937/625], Loss: 0.2347\n",
      "Epoch [213/250], Step [39937/625], Loss: 0.1819\n",
      "Epoch [214/250], Step [39937/625], Loss: 0.2503\n",
      "Epoch [215/250], Step [39937/625], Loss: 0.2374\n",
      "Epoch [216/250], Step [39937/625], Loss: 0.2415\n",
      "Epoch [217/250], Step [39937/625], Loss: 0.2220\n",
      "Epoch [218/250], Step [39937/625], Loss: 0.2409\n",
      "Epoch [219/250], Step [39937/625], Loss: 0.2191\n",
      "Epoch [220/250], Step [39937/625], Loss: 0.1919\n",
      "Epoch [221/250], Step [39937/625], Loss: 0.2350\n",
      "Epoch [222/250], Step [39937/625], Loss: 0.3058\n",
      "Epoch [223/250], Step [39937/625], Loss: 0.2440\n",
      "Epoch [224/250], Step [39937/625], Loss: 0.2530\n",
      "Epoch [225/250], Step [39937/625], Loss: 0.2247\n",
      "Epoch [226/250], Step [39937/625], Loss: 0.2514\n",
      "Epoch [227/250], Step [39937/625], Loss: 0.2583\n",
      "Epoch [228/250], Step [39937/625], Loss: 0.2528\n",
      "Epoch [229/250], Step [39937/625], Loss: 0.2640\n",
      "Epoch [230/250], Step [39937/625], Loss: 0.2462\n",
      "Epoch [231/250], Step [39937/625], Loss: 0.2037\n",
      "Epoch [232/250], Step [39937/625], Loss: 0.2182\n",
      "Epoch [233/250], Step [39937/625], Loss: 0.2364\n",
      "Epoch [234/250], Step [39937/625], Loss: 0.2913\n",
      "Epoch [235/250], Step [39937/625], Loss: 0.2544\n",
      "Epoch [236/250], Step [39937/625], Loss: 0.2260\n",
      "Epoch [237/250], Step [39937/625], Loss: 0.3113\n",
      "Epoch [238/250], Step [39937/625], Loss: 0.2192\n",
      "Epoch [239/250], Step [39937/625], Loss: 0.2291\n",
      "Epoch [240/250], Step [39937/625], Loss: 0.2542\n",
      "Epoch [241/250], Step [39937/625], Loss: 0.2457\n",
      "Epoch [242/250], Step [39937/625], Loss: 0.3040\n",
      "Epoch [243/250], Step [39937/625], Loss: 0.2589\n",
      "Epoch [244/250], Step [39937/625], Loss: 0.2300\n",
      "Epoch [245/250], Step [39937/625], Loss: 0.2577\n",
      "Epoch [246/250], Step [39937/625], Loss: 0.2837\n",
      "Epoch [247/250], Step [39937/625], Loss: 0.3014\n",
      "Epoch [248/250], Step [39937/625], Loss: 0.3123\n",
      "Epoch [249/250], Step [39937/625], Loss: 0.3996\n",
      "Epoch [250/250], Step [39937/625], Loss: 0.3295\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T20:05:18.995068Z",
     "start_time": "2024-06-29T20:05:17.386651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(0, len(test_dataset), batch_size):\n",
    "        x_batch, y_batch = test_dataset[i:i+batch_size]\n",
    "        x_batch = torch.tensor(x_batch).to(device).long()\n",
    "        y_batch = torch.tensor(y_batch).to(device).float()\n",
    "\n",
    "        outputs = model(x_batch).squeeze()  # Squeeze the output\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100*correct/total:.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, predicted.cpu()):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, predicted.cpu()):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, predicted.cpu()):.4f}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, predicted.cpu())}\")"
   ],
   "id": "1d3565bd3f32338c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.3700\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10000, 16]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 19\u001B[0m\n\u001B[0;32m     16\u001B[0m     correct \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (predicted \u001B[38;5;241m==\u001B[39m y_batch)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m100\u001B[39m\u001B[38;5;241m*\u001B[39mcorrect\u001B[38;5;241m/\u001B[39mtotal\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrecision: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprecision_score(y_test,\u001B[38;5;250m \u001B[39mpredicted\u001B[38;5;241m.\u001B[39mcpu())\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRecall: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrecall_score(y_test,\u001B[38;5;250m \u001B[39mpredicted\u001B[38;5;241m.\u001B[39mcpu())\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mF1 Score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mf1_score(y_test,\u001B[38;5;250m \u001B[39mpredicted\u001B[38;5;241m.\u001B[39mcpu())\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\PCN\\Seniment Analysis\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\Desktop\\PCN\\Seniment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2190\u001B[0m, in \u001B[0;36mprecision_score\u001B[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001B[0m\n\u001B[0;32m   2023\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m   2024\u001B[0m     {\n\u001B[0;32m   2025\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2050\u001B[0m     zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2051\u001B[0m ):\n\u001B[0;32m   2052\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute the precision.\u001B[39;00m\n\u001B[0;32m   2053\u001B[0m \n\u001B[0;32m   2054\u001B[0m \u001B[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2188\u001B[0m \u001B[38;5;124;03m    array([0.5, 1. , 1. ])\u001B[39;00m\n\u001B[0;32m   2189\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2190\u001B[0m     p, _, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mprecision_recall_fscore_support\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2191\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2192\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpos_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2195\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwarn_for\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprecision\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2197\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mzero_division\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mzero_division\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2199\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2200\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m p\n",
      "File \u001B[1;32m~\\Desktop\\PCN\\Seniment Analysis\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    184\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[1;32m--> 186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    188\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\PCN\\Seniment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1775\u001B[0m, in \u001B[0;36mprecision_recall_fscore_support\u001B[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001B[0m\n\u001B[0;32m   1612\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001B[39;00m\n\u001B[0;32m   1613\u001B[0m \n\u001B[0;32m   1614\u001B[0m \u001B[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1772\u001B[0m \u001B[38;5;124;03m array([2, 2, 2]))\u001B[39;00m\n\u001B[0;32m   1773\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1774\u001B[0m _check_zero_division(zero_division)\n\u001B[1;32m-> 1775\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[43m_check_set_wise_labels\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1777\u001B[0m \u001B[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001B[39;00m\n\u001B[0;32m   1778\u001B[0m samplewise \u001B[38;5;241m=\u001B[39m average \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msamples\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\Desktop\\PCN\\Seniment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1547\u001B[0m, in \u001B[0;36m_check_set_wise_labels\u001B[1;34m(y_true, y_pred, average, labels, pos_label)\u001B[0m\n\u001B[0;32m   1544\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m average \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m average_options \u001B[38;5;129;01mand\u001B[39;00m average \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1545\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maverage has to be one of \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(average_options))\n\u001B[1;32m-> 1547\u001B[0m y_type, y_true, y_pred \u001B[38;5;241m=\u001B[39m \u001B[43m_check_targets\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1548\u001B[0m \u001B[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001B[39;00m\n\u001B[0;32m   1549\u001B[0m \u001B[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001B[39;00m\n\u001B[0;32m   1550\u001B[0m present_labels \u001B[38;5;241m=\u001B[39m unique_labels(y_true, y_pred)\u001B[38;5;241m.\u001B[39mtolist()\n",
      "File \u001B[1;32m~\\Desktop\\PCN\\Seniment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:99\u001B[0m, in \u001B[0;36m_check_targets\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_targets\u001B[39m(y_true, y_pred):\n\u001B[0;32m     73\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001B[39;00m\n\u001B[0;32m     74\u001B[0m \n\u001B[0;32m     75\u001B[0m \u001B[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;124;03m    y_pred : array or indicator matrix\u001B[39;00m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 99\u001B[0m     \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    100\u001B[0m     type_true \u001B[38;5;241m=\u001B[39m type_of_target(y_true, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    101\u001B[0m     type_pred \u001B[38;5;241m=\u001B[39m type_of_target(y_pred, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_pred\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\PCN\\Seniment Analysis\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:460\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    458\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 460\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    461\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    462\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    463\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [10000, 16]"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T20:05:19.021061Z",
     "start_time": "2024-06-29T20:05:19.020061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plotting the loss graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(LossScore)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Epochs\")\n",
    "plt.show()"
   ],
   "id": "d8cbb9d6283bc7a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T20:05:19.028063Z",
     "start_time": "2024-06-29T20:05:19.025069Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "80eb5e0b115e0de3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
